{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d51f20c",
   "metadata": {},
   "source": [
    "This file downloads NOAA's StormEvents data from 1950 to 2024 and selects from the obtained dataframe the entries corresponding to relevant tornadoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cb406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing year: 1950\n",
      "\n",
      "Processing year: 1951\n",
      "\n",
      "Processing year: 1952\n",
      "\n",
      "Processing year: 1953\n",
      "\n",
      "Processing year: 1954\n",
      "\n",
      "Processing year: 1955\n",
      "\n",
      "Processing year: 1956\n",
      "\n",
      "Processing year: 1957\n",
      "\n",
      "Processing year: 1958\n",
      "\n",
      "Processing year: 1959\n",
      "\n",
      "Processing year: 1960\n",
      "\n",
      "Processing year: 1961\n",
      "\n",
      "Processing year: 1962\n",
      "\n",
      "Processing year: 1963\n",
      "\n",
      "Processing year: 1964\n",
      "\n",
      "Processing year: 1965\n",
      "\n",
      "Processing year: 1966\n",
      "\n",
      "Processing year: 1967\n",
      "\n",
      "Processing year: 1968\n",
      "\n",
      "Processing year: 1969\n",
      "\n",
      "Processing year: 1970\n",
      "\n",
      "Processing year: 1971\n",
      "\n",
      "Processing year: 1972\n",
      "\n",
      "Processing year: 1973\n",
      "\n",
      "Processing year: 1974\n",
      "\n",
      "Processing year: 1975\n",
      "\n",
      "Processing year: 1976\n",
      "\n",
      "Processing year: 1977\n",
      "\n",
      "Processing year: 1978\n",
      "\n",
      "Processing year: 1979\n",
      "\n",
      "Processing year: 1980\n",
      "\n",
      "Processing year: 1981\n",
      "\n",
      "Processing year: 1982\n",
      "\n",
      "Processing year: 1983\n",
      "\n",
      "Processing year: 1984\n",
      "\n",
      "Processing year: 1985\n",
      "\n",
      "Processing year: 1986\n",
      "\n",
      "Processing year: 1987\n",
      "\n",
      "Processing year: 1988\n",
      "\n",
      "Processing year: 1989\n",
      "\n",
      "Processing year: 1990\n",
      "\n",
      "Processing year: 1991\n",
      "\n",
      "Processing year: 1992\n",
      "\n",
      "Processing year: 1993\n",
      "\n",
      "Processing year: 1994\n",
      "\n",
      "Processing year: 1995\n",
      "\n",
      "Processing year: 1996\n",
      "\n",
      "Processing year: 1997\n",
      "\n",
      "Processing year: 1998\n",
      "\n",
      "Processing year: 1999\n",
      "\n",
      "Processing year: 2000\n",
      "\n",
      "Processing year: 2001\n",
      "\n",
      "Processing year: 2002\n",
      "\n",
      "Processing year: 2003\n",
      "\n",
      "Processing year: 2004\n",
      "\n",
      "Processing year: 2005\n",
      "\n",
      "Processing year: 2006\n",
      "\n",
      "Processing year: 2007\n",
      "\n",
      "Processing year: 2008\n",
      "\n",
      "Processing year: 2009\n",
      "\n",
      "Processing year: 2010\n",
      "\n",
      "Processing year: 2011\n",
      "\n",
      "Processing year: 2012\n",
      "\n",
      "Processing year: 2013\n",
      "\n",
      "Processing year: 2014\n",
      "\n",
      "Processing year: 2015\n",
      "\n",
      "Processing year: 2016\n",
      "\n",
      "Processing year: 2017\n",
      "\n",
      "Processing year: 2018\n",
      "\n",
      "Processing year: 2019\n",
      "\n",
      "Processing year: 2020\n",
      "\n",
      "Processing year: 2021\n",
      "\n",
      "Processing year: 2022\n",
      "\n",
      "Processing year: 2023\n",
      "\n",
      "Processing year: 2024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "base_url = \"https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "creation_date = \"20250520\"  \n",
    "all_years_data = []\n",
    "\n",
    "\n",
    "for year in range(1950, 2025):\n",
    "    print(f\"\\nProcessing year: {year}\")\n",
    "\n",
    "    if year == 1950:\n",
    "        date = 20250401\n",
    "    elif year == 2020:\n",
    "        date = 20240620\n",
    "    else:\n",
    "        date = creation_date\n",
    "    details_file = f\"StormEvents_details-ftp_v1.0_d{year}_c{date}.csv.gz\"\n",
    "\n",
    "\n",
    "    def download_csv_gz(file_name):\n",
    "        url = base_url + file_name\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()\n",
    "            return pd.read_csv(BytesIO(r.content), compression='gzip', low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    df_details = download_csv_gz(details_file)\n",
    "\n",
    "    if df_details is None:\n",
    "        continue\n",
    "\n",
    "    df_details.columns = df_details.columns.str.lower()\n",
    "\n",
    "    if 'event_id' not in df_details.columns:\n",
    "        continue\n",
    "\n",
    "    if year >= 1950 and year <= 2007:\n",
    "        f_list = ['F0', 'F1', 'F2', 'F3', 'F4', 'F5']\n",
    "    else:\n",
    "        f_list = ['EF0', 'EF1', 'EF2', 'EF3', 'EF4', 'EF5']\n",
    "    df_tornadoes = df_details[\n",
    "    (df_details['event_type'] == 'Tornado') &\n",
    "    (df_details['tor_f_scale'].isin(f_list))].copy()\n",
    "\n",
    "    df_tornadoes['year'] = year  \n",
    "\n",
    "\n",
    "    if df_tornadoes.empty:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    df_tornadoes['year'] = year\n",
    "    all_years_data.append(df_tornadoes[['state', 'begin_lat', 'begin_lon', 'tor_f_scale', 'year']])\n",
    "\n",
    "\n",
    "combined_df = pd.concat(all_years_data, ignore_index=True)\n",
    "\n",
    "def year_to_bin(y):\n",
    "    base = y - (y % 4)\n",
    "    return f\"{base}-{base + 3}\"\n",
    "\n",
    "combined_df['year_bin'] = combined_df['year'].apply(year_to_bin)\n",
    "\n",
    "\n",
    "#binned_summary = combined_df.groupby(['year_bin', 'state', 'begin_lat', 'begin_lon', 'tor_f_scale']) \\\n",
    "#                            .size().reset_index(name='count')\n",
    "\n",
    "\n",
    "#binned_summary.to_csv(\"1952_2007.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7826eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('all_tornadoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32632052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErdosInstituteSummer2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
